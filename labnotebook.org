
Archived entries from file /Users/ian/ciencia/projects_202109_drosophila-sweeps/todo.org

* 2021

** 2021-09 September

*** 2021-09-30 Thursday
**** DONE [#B] Simulate with log-scale normalized statistics. [100%]
CLOSED: [2021-09-30 Thu 18:39] SCHEDULED: <2021-09-30 Thu>
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-04 Mon 11:31
:ARCHIVE_FILE: ~/ciencia/projects_202109_drosophila-sweeps/todo.org
:ARCHIVE_OLPATH: Sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:END:

- State "DONE"       from "DOING"      [2021-09-30 Thu 18:39]
- State "DOING"      from "TODO"       [2021-09-30 Thu 13:03]
- [X] Check if simulations are saving a log-transformed .tar of data.
- [X] If not, implement that in simulations: make a folder of unique npys with the log data, then tar that.
- [X] Test the new stuff locally.
- [X] Check if cluster scripts are saving the .tar of log-transformed data.
- [X] If not, make them save that as well.
- [X] Git update everything.
- [X] Move existing cluster simulations to their own folder, create a new one for raw simulations.
- [X] Now we just need to re-run cluster simulations, yet again. Do this *after* debugging the new simulation results.

**** DONE [#A] Debug feature calculation failure with zero segregating sites. [100%]
CLOSED: [2021-09-30 Thu 16:56] SCHEDULED: <2021-09-30 Thu>
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-04 Mon 11:31
:ARCHIVE_FILE: ~/ciencia/projects_202109_drosophila-sweeps/todo.org
:ARCHIVE_OLPATH: Sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:END:

- State "DONE"       from "TODO"       [2021-09-30 Thu 16:56]
- [X] Add a config with mutation rate of zero to produce zero segregating sites all the time.
- [X] Change =02_run= to use that config for now.
- [X] Check that we get the same =genfromtxt= error using that config.
- [X] Reproduce =genfromtxt= error in an interactive session.
- [X] What is the error caused by?
- [X] Fix it in an interactive session.
- [X] Confirm that we read in 0 segregating sites properly.
- [X] Confirm that we read in 1 segregating site properly.
- [X] Confirm that we read in >1 segregating sites properly.
- [X] Check timing of new function vs. old function for >1 segregating sites.
- [X] Put new function in the script.
- [X] Make feature calculation work with 0 segregating sites.
- [X] Run simulations and check that everything runs, and features are calculated properly.
- [X] Clean output folder of simulations.
- [X] Change =02_run= back to its normal state.
- [X] Put zero-mutations simulation in as a simtest folder.
- [X] Git update everything.


** 2021-10 October

*** 2021-10-01 Friday
**** DONE [#A] Debug new simulation results. [100%]
CLOSED: [2021-10-01 Fri 11:16] SCHEDULED: <2021-09-30 Thu>
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-04 Mon 11:31
:ARCHIVE_FILE: ~/ciencia/projects_202109_drosophila-sweeps/todo.org
:ARCHIVE_OLPATH: Sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:END:

- State "DONE"       from "DOING"      [2021-10-01 Fri 11:16]
- State "DOING"      from "TODO"       [2021-09-30 Thu 13:03]
- My hypothesis was that all SLURM tasks from a single array job are happening in the same workdir, and thus the snakemake input/output calculations are going haywire.
- It turns out that the =/workdir= scratch space on =cbsubscb10= was 100% full, so you couldn't =mkdir= a folder there; I was getting this mysterious "Input/output error". So instead of creating a directory in scratch and switching to it, we were running simulations from inside home in multiple jobs and tasks, causing a huge amount of race conditions. The temporary solution is to avoid =cbsubscb10= for now and hope no other compute nodes have the same problem.
- [X] Wait for everything to run in the cluster.
- [X] Download the tars.
- [X] Check the SLURM output for the logs.
- [X] Does every data tar contain the same NPY files? No...
- [X] Change name of output file to include slurm job array ID. Locally.
- [X] Change workdir to include slurm job array ID. Locally.
- [X] slurm array task id vs. slurm array job id.
- [X] Git update just the slurm script.
- [X] Clean cluster folder of slurm outputs.
- [X] Check workdirs.
- [X] Confirm cluster slurm output are alright.
- [X] Re-run with very few simulations jobs; comment original amounts out.
- [X] Confirm it runs properly.
- [X] Confirm there are not extra files in =~/drosophila-sweeps=.
- [X] Git checkout of the slurm sbatch script so simulation amounts are back to normal.
- [X] Update git from the cluster: fetch, merge, then commit and push.
- [X] Delete local simulations.
- [X] Read SLURM outputs to see what failed.
- [X] Make testing SLURM scripts.
- [X] See if I run into the same mistake as before.
- [X] Re-run simulations with only a couple of runs.
- [X] Clean up =benchmark=, =output= and =bin= from the main folder.
- [X] Do not use cbsubscb10.
- [X] Try re-running the big ones now.

*** 2021-10-05 Tuesday
**** DONE Combine simulations, including log-transformed data. [100%]
CLOSED: [2021-10-05 Tue 15:34] SCHEDULED: <2021-10-04 Mon>
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-27 Wed 11:29
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:END:
  
- State "DONE"       from "DOING"      [2021-10-05 Tue 15:34]
- State "DOING"      from "TODO"       [2021-10-04 Mon 18:40]
- [X] Grep the slurm outputs for "Input/output" errors.
- [X] Check if the main folder does not have an =output=, etc.
- [X] Wait for everything to run.
- [X] Check that the log-transformed =tar= of data are there.
- [X] Remove old simulations.
- [X] Rename folder of new simulations to =raw-simulations=.
- [X] Download new data.
- [X] Replace current data with new data that includes log-transformed data.
- [X] Additional rule for combining simulation results, but using the log-transformed data this time. This can be a simple parameter passed to the simulation combination notebook.
- [X] Make features and logdata outputs of the simulation combination rule.
- [X] Modify =03_config.yaml= back to using real data.
- [X] Run everything to combine data.
- [X] Confirm that things ran as planned.

*** 2021-10-18 Monday
**** DONE Snakemake rules for model training. [100%]
CLOSED: [2021-10-18 Mon 12:35]
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-27 Wed 11:29
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:END:

- State "DONE"       from "DOING"      [2021-10-18 Mon 12:35] \\
  Brainstormed what the Snakemake pipeline for training models would look like. Still missing: the pipeline for converting inferences on training, validation, and testing datasets into performance metrics.
- State "DOING"      from "TODO"       [2021-10-18 Mon 12:00]
- [X] Use the fastai conda environment.
- [X] Read the code for building a model. What steps happen in it?
- [X] Read the code for training a model. What steps happen in it?
- [X] How would we turn these steps into rules with inputs and outputs?
- [X] List in config for datasets to train models with.
- [X] A general rule for training a model and outputting trained model, reports of fitting, and validation results.
- [X] One rule per target of inference. 
- [X] Use multiple cores when training, add number of cores to the snakemake rule as well.
- [X] Use a smaller dataset for training.

*** 2021-10-20 Wednesday
**** DONE Toy model training implementation. [100%]
CLOSED: [2021-10-20 Wed 19:00]
:PROPERTIES:
:ARCHIVE_TIME: 2021-10-27 Wed 11:29
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:END:

- State "DONE"       from "DOING"      [2021-10-20 Wed 19:00]
- State "DOING"      from "TODO"       [2021-10-20 Wed 16:01]
- [X] Add lists of training and testing datasets to config; add list of inference targets to config.
- [X] Expand those lists into the final outputs of the process, the testing inferences. Add that into the top rule.
- [X] Write toy Test rule with =touch=.
- [X] Write toy Fit rule with =touch=.
- [X] Write toy Split rule with =touch=.
- [X] Write toy Balance rule with =touch=.
- [X] Add outcomes of overfitting analysis to top rule. Try and keep that in as few files as possible.
- [X] Add toy rule to combine SimpleFit outcomes with =touch=.
- [X] Add toy SimpleFit rule with =touch=.

*** 2021-10-27 Wednesday
**** DONE Skeleton training implementations. [100%]
CLOSED: [2021-10-27 Wed 13:18]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-01 Mon 15:01
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "DOING"      [2021-10-27 Wed 13:18]
- State "DOING"      from "TODO"       [2021-10-27 Wed 12:27]
Convert rules into notebooks or scripts that load the right packages, produce out the right files, and use the right number of cores, instead of simply using =touch=.

- [X] Balance rule. [7/7]
  - [X] Import pandas.
  - [X] Dictionary of balancing function per target.
  - [X] Current target is a wildcard.
  - [X] Produce one output only, the one for the target.
  - [X] Skeleton functions in =prepare_data.py=.
  - [X] Delete outputs and run test.
  - [X] Columns have mixed types?
- [X] Split rule. [10/10]
  - [X] Close notebook, re-run with new conda env.
  - [X] Check sklearn version; add that to the env.
  - [X] Close notebook again, re-run again with new env.
  - [X] Now complete the notebook: stratify, use random seed, shuffle.
  - [X] Turn validation % into a snakemake parameter.
  - [X] Put stratifying column in =prepare_data= and import that into notebook.
  - [X] Run notebook to see the files appear.
  - [X] Check label proportions and sizes of outputs.
  - [X] Convert notebook back to using the proper validation % before closing it.
  - [X] Re-run, forcing the rule but not editing the notebook.
- [X] Fit rule. [2/2]
  - [X] Import fastai.
  - [X] Produce every output in the most basic way possible.
- [X] SimpleFit rule. [3/3]
  - [X] Variation of Fit notebook, but with a different parameter.
  - [X] Run snakemake with the target being a fit replicate.
  - [X] Make a toy 1-line DF for the fit report instead of a touch.
- [X] Aggregating overfitting replicates rule. [2/2]
  - [X] Script, not notebook.
  - [X] Use simple pandas concat.
- [X] Update git and github.

*** 2021-10-28 Thursday
**** DONE Split training and validation at the start. [100%]
CLOSED: [2021-10-28 Thu 16:00]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-01 Mon 15:01
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:
- State "DONE"       from "DOING"      [2021-10-28 Thu 16:00]
- State "DOING"      from              [2021-10-27 Wed 15:56]
- [X] Write question down to understand the issue yourself.
- [X] Ask about differences.
- [X] Is it interesting to do?
- [X] Plan reworking of rules.
- [X] Implement reworking in terms of Snakefile wildcards and rules.
  - [X] Train/valid split is done once per training data, as the first thing. Output folder.
  - [X] Balancing rule takes the train/valid splits as input, happens once per training data and once per target. Output folder.
  - [X] Fitting rules (both) take in the balanced datasets as input.
- [X] Confirm with =-n= that all necessary files are there.
- [X] Produce a DAG in PDF format with dot. (Check bash history to see how.)
- [X] Implement reworking in terms of skeleton notebooks.
  - [X] Train/valid split.
  - [X] Balancing.
  - [X] Fitting.
- [X] Delete current outputs, test one output file to see if it works.
- [X] Git update.
  
I have a main dataset with labels A, B, C in equal proportions. I want to train two models: model 1 is a 3-way classification A vs. B vs. C, and model 2 is a binary classification A vs. (B + C). For that second one labels B and C are downsampled so (num A) = (num B + C). I can do training/testing split in two ways:

1) Do it once, at the start, so I've got a fixed A+B+C training dataset and a fixed A+B+C testing set. For the binary model, downsample those training and testing sets. This way, any example x_i is always going to be in the training set for both models or the testing set for both models.

2) Do it one time per model. Do a training/testing split for model 1. Then, downsample the data and do a separate training/testing split for model 2. This way, an example x_i that's training for model 1 may be testing for model 2.

I don't have a great intuition for what's "best" among these. Intuition tells me option (1) might propagate any weird bias you might end up having by chance in your original train/test split to every model. While option (2) makes it harder to compare the models. Any thoughts?

** 2021-11 November

*** 2021-11-10 Wednesday
**** DONE [#3] New plot of selection strength performance. [100%]
CLOSED: [2021-11-10 Wed 12:00]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-11-10 Wed 12:00]
- State "DONE"       from "TODO"       [2021-11-09 Tue 12:19]
- [X] Find notebook with the right plotting code.
- [X] Reconstruct plot with both sel. strength and sweep mode performance.
- [X] Load dataframe with all s predictions for the validation set.
- [X] Calculate (predicted s/true s), not in log scale, for every data point.
- [X] Get a list of all selection splits, with separate values for min and max.
- [X] For each selection split, calculate the mean of that value.
- [X] Create a DF with split and mean relative error.
- [X] Join that DF with the original one from the notebook.
- [X] Plot the mean relative error by selection bracket.
- [X] Plot mean(abs((predicted - true)/true)) instead.
- [X] What are the conclusions?
- [X] Send that to Philipp.

Sel. strength performance vs. sel. strength bracket: Instead of showing 1-RMSE, show mean(predicted s/true s). (Not in log scale!) There is a thing called "root mean squared relative error". Philipp would call it "mean relative error". Philipp thinks this would turn results around and say that we do better at strong sweeps than weak ones.


**** DONE [#3] Finalize skeleton model fitting implementations. [100%]
CLOSED: [2021-11-10 Wed 21:13]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "DOING"      [2021-11-10 Wed 21:13]
- State "DOING"      from "TODO"       [2021-11-10 Wed 16:30]
- [X] First workflow modifications. [10/10]
  - [X] Modify existing rule or add rule?
  - [X] Tar empirical windows.
  - [X] Test that entire workflow works with tar windows.
  - [X] Add log windows to empirical features rule.
  - [X] Implement log windows in notebook.
  - [X] Add a waiting rule for log-tar windows.
  - [X] Tar log empirical windows.
  - [X] Add log windows to the =all= rule.
  - [X] Test that the entire workflow still works.
  - [X] Git update.
- [X] Test application rules in workflow 3. [4/4]
  - [X] Same notebook of trained model application for both.
  - [X] For now, just touching outputs is fine.
  - [X] Test that everything works fine.
  - [X] Git update.
- [X] Delete outputs and run everything in workflow 3. [5/5]
  - [X] Training data, model fitting, trained models, all inferences.
  - [X] Add benchmarks and notebook logs for everything.
  - [X] Add wildcards to benchmarks and logs.
  - [X] Test that everything works fine.
  - [X] Git update.
- [X] Every benchmark for workflow 1 goes into a =prepare= folder.
- [X] Every benchmark for workflow 2 goes into a separate folder as well.

*** 2021-11-11 Thursday
**** DONE [#3] Re-draw training and validation datasets for overfitting replicates. [100%] 
CLOSED: [2021-11-11 Thu 14:34]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:
- State "DONE"       from "DOING"      [2021-11-11 Thu 14:34]
- State "DOING"      from "TODO"       [2021-11-11 Thu 14:32]
- [X] New required inputs.
- [X] Can we just use the existing rule?
- [X] Add rules if needed.
- [X] Adapt notebook if necessary. (=params.random_seed= is optional.)
- [X] Double-check benchmarks and logs of overfitting rules.
- [X] Run everything, test.

**** DONE [#3] Train-test splitting notebook. [100%]
CLOSED: [2021-11-11 Thu 15:28]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-11-11 Thu 15:28]
- [X] Remove lines corresponding to failed simulations.
- [X] Remove neutral simulations.
- [X] Downsample so all four regimes are equally represented. Possibly so that they all have exactly 4000 cases for training and 1000 for validation.

*** 2021-11-12 Friday
**** DONE [#3] Rule for cleaning parameters. [100%]
CLOSED: [2021-11-12 Fri 11:12]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-11-12 Fri 11:12]
Since the train-test split runs once for every overfitting replicate, if we downsample the data here, each replicate will get a different dataset of 5000 sweeps for each mode. We want each replicate to have the same dataset. Meaning we need to add a new rule between getting the parameters and splitting them into training and testing datasets.

- [X] Add a rule for cleaning parameters before the train-test split. Should run once per training dataset.
- [X] Update input of rule for checking the training distribution of cleaned simulations.
- [X] Rule removes failed simulations, neutral simulations, and downsamples to a fixed number of cases per selection regime (5000, make it a snakemake parameter).
- [X] Move that code over from the current notebook on train-test split to a script.
- [X] Change input of the train-valid split to accomodate the new rule.
- [X] Change input of the overfitting splitting rule to accomodate the new rule as well.
- [X] Confirm that train-test split is shuffling cases.
- [X] Test it, check that it works.
- [X] Updage git.

**** DONE [#3] Balancing functions. [100%]
CLOSED: [2021-11-12 Fri 15:42]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-11-12 Fri 15:42] \\
  Changed my mind, there's a lot of changes to make.
Should: Add necessary columns.
Should: Downsample to match.

- [X] Balance log selection strength.
- [X] Balance sweep mode.
- [X] Balance hard vs. soft.
- [X] Balance SGV vs. RNM.
- [X] Move code back to script.
- [X] Complete the "target columns".
- [X] Compare with previous data preparation functions.
- [X] Create column of "true softness" when balancing the data.
- [X] Redo target columns and balancing functions to account for true softness.

*** 2021-11-23 Tuesday
**** DONE [#3] Number of simulations. [100%]
CLOSED: [2021-11-23 Tue 12:56]
:PROPERTIES:
:ARCHIVE_TIME: 2021-11-23 Tue 17:48
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-11-23 Tue 12:56]
- State "DOING"      from "TODO"       [2021-11-16 Tue 15:30]
  
We start with a training that has:
- Failed simulations.
- Neutral simulations.
- Soft sweeps that aren't really soft, that only have 1 adaptive allele.
We need to get rid of all these things for training data.
We want to /add/:
- Sweep mode with true soft sweeps.
- Log of selection coefficient.
And after this, we want to make training data 4000+1000 samples.
For robustness datasets, you want to /add/ the columns but remove only the failed simulations, since neutrality and hard-like soft sweeps are all informative. We should have columns that indicate their existence, though.

So this is universal, for every single dataset:
1. Remove failed simulations.
2. Make a column for sweep mode: neutral, hard, true sgv, true rnm, fake sgv, fake rnm.
3. Make a column for log selection coefficient.
4. Report how many successful and failed simulations are there for each sweep mode.
5. Report distributions of successful and failed simulations per sweep mode.

For training and validation data:
1. Remove neutral, fake sgv, and fake rnm sweep modes.
2. Save those removed cases for a full validation dataset.
3. Subset to 5000 cases per mode.

My work so far is in =notebooks/inference/Untitled.ipynb=.

- [X] Plan data preparation for /training/ and /validation/ datasets, write it all down.
- [X] Plan data preparation for /robustness/ datasets, write it all down.
- Universal rules are applied in the =clean= rule:
  - [X] Adapt =make-data-report= for the case when there is only one simulation status.
  - [X] Fold all of =make-data-report= into a single function.
  - [X] Move that into the =clean= script.
  - [X] Invoke it separately for the successful and the failed simulations.
  - [X] Add all those reports to the =clean= rule outputs.
  - [X] Test it all.
- Train/validation rules are applied in the =split= rule:
  - [X] Remove neutral, fake sgv, and fake rnm sweep modes.
  - [X] Save those removed cases for a full validation dataset.
  - [X] Subset to 5000 cases per mode.
- [X] Make the useful functions easy to reuse by putting them in utils.
- [X] Transfer tain/valid split to a script.
- [X] What to do with the =balancing= rule?
- [X] Check workflow.
- [X] Check that balanced training datasets have equal number of target cases.
- [X] Clean unused notebooks.
- [X] How many simulations have we got left? 4000 + 1000?
- [X] Black every script.
- [X] Update git.

*** 2021-11-24 Wednesday
**** DONE [#1] Read about Python's tar package.
CLOSED: [2021-11-24 Wed 16:10]
:PROPERTIES:
:ARCHIVE_TIME: 2021-12-21 Tue 12:44
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:
- State "DONE"       from "DOING"      [2021-11-24 Wed 16:10]
- State "DOING"      from "TODO"       [2021-11-24 Wed 16:07]
- Module of the Week.
- Official documentation.

**** DONE [#3] Simulate more data. [5/5]
CLOSED: [2021-11-24 Wed 15:45]
:PROPERTIES:
:ARCHIVE_TIME: 2021-12-21 Tue 12:44
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "DOING"      [2021-11-24 Wed 15:45]
- State "DOING"      from "TODO"       [2021-11-24 Wed 12:58]
Around 50% of RNM sweeps are hard-like.
Around 11% of SGV sweeps are hard-like.

To get 5000 true sweeps of each, I need more training simulations.

- [X] Update github repo, locally and on the cluster.
- [X] Change simulation amounts.
- [X] Double-check simulation destiation folders.
- [X] Run simulations.
- [X] Check that simulations are running okay.


** 2021-12 December

*** 2021-12-01 Wednesday
**** DONE [#3] Get new simulations. [2/2]
CLOSED: [2021-12-01 Wed 14:25]
:PROPERTIES:
:ARCHIVE_TIME: 2021-12-21 Tue 12:44
:ARCHIVE_FILE: ~/pessoal/essencial/agenda.org
:ARCHIVE_CATEGORY: PhD
:ARCHIVE_TODO: DONE
:ARCHIVE_ITAGS: project
:ARCHIVE_OLPATH: PhD/Drosophila sweeps
:END:

- State "DONE"       from "TODO"       [2021-12-01 Wed 14:25]
- [X] Check if simulations ran okay in the cluster.
- [X] Download them.
  
